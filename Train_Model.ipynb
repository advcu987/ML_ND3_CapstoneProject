{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 151293 \n",
      "Validation size: 35876 \n",
      "Test size: 35659\n"
     ]
    }
   ],
   "source": [
    "# Source: \n",
    "# https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/\n",
    "\n",
    "# Applying Transforms to the Data\n",
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "#         transforms.RandomRotation(degrees=15),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "# Set train and valid directory paths\n",
    "train_directory = '/home/advo/PycharmProjects/ML_ND3_CapstoneProject/Dataset_small/patches/train'\n",
    "valid_directory = '/home/advo/PycharmProjects/ML_ND3_CapstoneProject/Dataset_small/patches/valid'\n",
    "test_directory = '/home/advo/PycharmProjects/ML_ND3_CapstoneProject/Dataset_small/patches/test'\n",
    " \n",
    "# Batch size\n",
    "bs = 64\n",
    " \n",
    "# Number of classes\n",
    "num_classes = 2\n",
    " \n",
    "# Load Data from folders\n",
    "data = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid']),\n",
    "    'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n",
    "}\n",
    " \n",
    "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
    "train_data_size = len(data['train'])\n",
    "valid_data_size = len(data['valid'])\n",
    "test_data_size = len(data['test'])\n",
    " \n",
    "# Create iterators for the Data loaded using DataLoader module\n",
    "train_data = torch.utils.data.DataLoader(data['train'], batch_size=bs, shuffle=True)\n",
    "valid_data = torch.utils.data.DataLoader(data['valid'], batch_size=bs, shuffle=True)\n",
    "test_data = torch.utils.data.DataLoader(data['test'], batch_size=bs, shuffle=True)\n",
    " \n",
    "# Print the train, validation and test set data sizes\n",
    "print(f\"Train size: {train_data_size} \\nValidation size: {valid_data_size} \\nTest size: {test_data_size}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained AlexNet Model\n",
    "alexnet = torchvision.models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /home/advo/.cache/torch/hub/checkpoints/densenet161-8d451a50.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "densenet = torchvision.models.densenet161(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model (alexnet/densenet)\n",
    "model = densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the parameters for the pretrained part\n",
    "# Source: https://pytorch.org/docs/master/notes/autograd.html\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: both models must be loaded, for this to work\n",
    "if model == densenet:\n",
    "    \n",
    "    # Update the classifier layer, in order to output the required number of classes specific to our problem\n",
    "    # Note: see model.eval() for details of each layer\n",
    "    model.classifier = nn.Linear(in_features=2208, out_features=num_classes, bias=True)\n",
    "\n",
    "else:\n",
    "    \n",
    "    # Source: \n",
    "    # https://analyticsindiamag.com/implementing-alexnet-using-pytorch-as-a-transfer-learning-model-in-multi-class-classification/\n",
    "    # Updating the second classifier(reduce the number of outputs, to prevent overfitting)\n",
    "    alexnet.classifier[4] = nn.Linear(4096,1024)\n",
    "\n",
    "    # Updating the third and the last classifier that is the output layer of the network\n",
    "    # Binary classification , thus only 2 output nodes\n",
    "    alexnet.classifier[6] = nn.Linear(1024, num_classes)\n",
    "\n",
    "model_name = model.__class__.__name__\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function: Cross Entropy Loss\n",
    "\n",
    "Note: Improvement option by adding weight class check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: \n",
    "# https://github.com/choosehappy/PytorchDigitalPathology/blob/master/visualization_densenet/train_densenet.ipynb\n",
    "\n",
    "# we have the ability to weight individual classes, in this case we'll do so based on their presense in the trainingset\n",
    "# to avoid biasing any particular class\n",
    "# nclasses = dataset[\"train\"].classsizes.shape[0]\n",
    "# class_weight=dataset[\"train\"].classsizes\n",
    "# class_weight = torch.from_numpy(1-class_weight/class_weight.sum()).type('torch.FloatTensor').to(device)\n",
    "\n",
    "# print(class_weight) #show final used weights, make sure that they're reasonable before continouing\n",
    "\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss(weight = class_weight)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: \n",
    "# https://github.com/choosehappy/PytorchDigitalPathology/blob/master/visualization_densenet/train_densenet.ipynb\n",
    "\n",
    "# adam is going to be the most robust, though perhaps not the best performing, typically a good place to start\n",
    "optimizer = optim.Adam(model.parameters()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(10):  # loop over the dataset multiple times\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(train_data, 0):\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         output = alexnet(inputs)\n",
    "#         loss = criterion(output, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training of AlexNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running on the following device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"You are running on the following device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print model and optimizer parameters before training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "features.conv0.weight \t torch.Size([96, 3, 7, 7])\n",
      "features.norm0.weight \t torch.Size([96])\n",
      "features.norm0.bias \t torch.Size([96])\n",
      "features.norm0.running_mean \t torch.Size([96])\n",
      "features.norm0.running_var \t torch.Size([96])\n",
      "features.norm0.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock1.denselayer1.norm1.weight \t torch.Size([96])\n",
      "features.denseblock1.denselayer1.norm1.bias \t torch.Size([96])\n",
      "features.denseblock1.denselayer1.norm1.running_mean \t torch.Size([96])\n",
      "features.denseblock1.denselayer1.norm1.running_var \t torch.Size([96])\n",
      "features.denseblock1.denselayer1.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock1.denselayer1.conv1.weight \t torch.Size([192, 96, 1, 1])\n",
      "features.denseblock1.denselayer1.norm2.weight \t torch.Size([192])\n",
      "features.denseblock1.denselayer1.norm2.bias \t torch.Size([192])\n",
      "features.denseblock1.denselayer1.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock1.denselayer1.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock1.denselayer1.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock1.denselayer1.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock1.denselayer2.norm1.weight \t torch.Size([144])\n",
      "features.denseblock1.denselayer2.norm1.bias \t torch.Size([144])\n",
      "features.denseblock1.denselayer2.norm1.running_mean \t torch.Size([144])\n",
      "features.denseblock1.denselayer2.norm1.running_var \t torch.Size([144])\n",
      "features.denseblock1.denselayer2.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock1.denselayer2.conv1.weight \t torch.Size([192, 144, 1, 1])\n",
      "features.denseblock1.denselayer2.norm2.weight \t torch.Size([192])\n",
      "features.denseblock1.denselayer2.norm2.bias \t torch.Size([192])\n",
      "features.denseblock1.denselayer2.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock1.denselayer2.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock1.denselayer2.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock1.denselayer2.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock1.denselayer3.norm1.weight \t torch.Size([192])\n",
      "features.denseblock1.denselayer3.norm1.bias \t torch.Size([192])\n",
      "features.denseblock1.denselayer3.norm1.running_mean \t torch.Size([192])\n",
      "features.denseblock1.denselayer3.norm1.running_var \t torch.Size([192])\n",
      "features.denseblock1.denselayer3.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock1.denselayer3.conv1.weight \t torch.Size([192, 192, 1, 1])\n",
      "features.denseblock1.denselayer3.norm2.weight \t torch.Size([192])\n",
      "features.denseblock1.denselayer3.norm2.bias \t torch.Size([192])\n",
      "features.denseblock1.denselayer3.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock1.denselayer3.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock1.denselayer3.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock1.denselayer3.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock1.denselayer4.norm1.weight \t torch.Size([240])\n",
      "features.denseblock1.denselayer4.norm1.bias \t torch.Size([240])\n",
      "features.denseblock1.denselayer4.norm1.running_mean \t torch.Size([240])\n",
      "features.denseblock1.denselayer4.norm1.running_var \t torch.Size([240])\n",
      "features.denseblock1.denselayer4.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock1.denselayer4.conv1.weight \t torch.Size([192, 240, 1, 1])\n",
      "features.denseblock1.denselayer4.norm2.weight \t torch.Size([192])\n",
      "features.denseblock1.denselayer4.norm2.bias \t torch.Size([192])\n",
      "features.denseblock1.denselayer4.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock1.denselayer4.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock1.denselayer4.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock1.denselayer4.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock1.denselayer5.norm1.weight \t torch.Size([288])\n",
      "features.denseblock1.denselayer5.norm1.bias \t torch.Size([288])\n",
      "features.denseblock1.denselayer5.norm1.running_mean \t torch.Size([288])\n",
      "features.denseblock1.denselayer5.norm1.running_var \t torch.Size([288])\n",
      "features.denseblock1.denselayer5.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock1.denselayer5.conv1.weight \t torch.Size([192, 288, 1, 1])\n",
      "features.denseblock1.denselayer5.norm2.weight \t torch.Size([192])\n",
      "features.denseblock1.denselayer5.norm2.bias \t torch.Size([192])\n",
      "features.denseblock1.denselayer5.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock1.denselayer5.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock1.denselayer5.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock1.denselayer5.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock1.denselayer6.norm1.weight \t torch.Size([336])\n",
      "features.denseblock1.denselayer6.norm1.bias \t torch.Size([336])\n",
      "features.denseblock1.denselayer6.norm1.running_mean \t torch.Size([336])\n",
      "features.denseblock1.denselayer6.norm1.running_var \t torch.Size([336])\n",
      "features.denseblock1.denselayer6.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock1.denselayer6.conv1.weight \t torch.Size([192, 336, 1, 1])\n",
      "features.denseblock1.denselayer6.norm2.weight \t torch.Size([192])\n",
      "features.denseblock1.denselayer6.norm2.bias \t torch.Size([192])\n",
      "features.denseblock1.denselayer6.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock1.denselayer6.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock1.denselayer6.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock1.denselayer6.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.transition1.norm.weight \t torch.Size([384])\n",
      "features.transition1.norm.bias \t torch.Size([384])\n",
      "features.transition1.norm.running_mean \t torch.Size([384])\n",
      "features.transition1.norm.running_var \t torch.Size([384])\n",
      "features.transition1.norm.num_batches_tracked \t torch.Size([])\n",
      "features.transition1.conv.weight \t torch.Size([192, 384, 1, 1])\n",
      "features.denseblock2.denselayer1.norm1.weight \t torch.Size([192])\n",
      "features.denseblock2.denselayer1.norm1.bias \t torch.Size([192])\n",
      "features.denseblock2.denselayer1.norm1.running_mean \t torch.Size([192])\n",
      "features.denseblock2.denselayer1.norm1.running_var \t torch.Size([192])\n",
      "features.denseblock2.denselayer1.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer1.conv1.weight \t torch.Size([192, 192, 1, 1])\n",
      "features.denseblock2.denselayer1.norm2.weight \t torch.Size([192])\n",
      "features.denseblock2.denselayer1.norm2.bias \t torch.Size([192])\n",
      "features.denseblock2.denselayer1.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock2.denselayer1.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock2.denselayer1.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer1.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock2.denselayer2.norm1.weight \t torch.Size([240])\n",
      "features.denseblock2.denselayer2.norm1.bias \t torch.Size([240])\n",
      "features.denseblock2.denselayer2.norm1.running_mean \t torch.Size([240])\n",
      "features.denseblock2.denselayer2.norm1.running_var \t torch.Size([240])\n",
      "features.denseblock2.denselayer2.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer2.conv1.weight \t torch.Size([192, 240, 1, 1])\n",
      "features.denseblock2.denselayer2.norm2.weight \t torch.Size([192])\n",
      "features.denseblock2.denselayer2.norm2.bias \t torch.Size([192])\n",
      "features.denseblock2.denselayer2.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock2.denselayer2.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock2.denselayer2.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer2.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock2.denselayer3.norm1.weight \t torch.Size([288])\n",
      "features.denseblock2.denselayer3.norm1.bias \t torch.Size([288])\n",
      "features.denseblock2.denselayer3.norm1.running_mean \t torch.Size([288])\n",
      "features.denseblock2.denselayer3.norm1.running_var \t torch.Size([288])\n",
      "features.denseblock2.denselayer3.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer3.conv1.weight \t torch.Size([192, 288, 1, 1])\n",
      "features.denseblock2.denselayer3.norm2.weight \t torch.Size([192])\n",
      "features.denseblock2.denselayer3.norm2.bias \t torch.Size([192])\n",
      "features.denseblock2.denselayer3.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock2.denselayer3.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock2.denselayer3.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer3.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock2.denselayer4.norm1.weight \t torch.Size([336])\n",
      "features.denseblock2.denselayer4.norm1.bias \t torch.Size([336])\n",
      "features.denseblock2.denselayer4.norm1.running_mean \t torch.Size([336])\n",
      "features.denseblock2.denselayer4.norm1.running_var \t torch.Size([336])\n",
      "features.denseblock2.denselayer4.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer4.conv1.weight \t torch.Size([192, 336, 1, 1])\n",
      "features.denseblock2.denselayer4.norm2.weight \t torch.Size([192])\n",
      "features.denseblock2.denselayer4.norm2.bias \t torch.Size([192])\n",
      "features.denseblock2.denselayer4.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock2.denselayer4.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock2.denselayer4.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer4.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock2.denselayer5.norm1.weight \t torch.Size([384])\n",
      "features.denseblock2.denselayer5.norm1.bias \t torch.Size([384])\n",
      "features.denseblock2.denselayer5.norm1.running_mean \t torch.Size([384])\n",
      "features.denseblock2.denselayer5.norm1.running_var \t torch.Size([384])\n",
      "features.denseblock2.denselayer5.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer5.conv1.weight \t torch.Size([192, 384, 1, 1])\n",
      "features.denseblock2.denselayer5.norm2.weight \t torch.Size([192])\n",
      "features.denseblock2.denselayer5.norm2.bias \t torch.Size([192])\n",
      "features.denseblock2.denselayer5.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock2.denselayer5.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock2.denselayer5.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer5.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock2.denselayer6.norm1.weight \t torch.Size([432])\n",
      "features.denseblock2.denselayer6.norm1.bias \t torch.Size([432])\n",
      "features.denseblock2.denselayer6.norm1.running_mean \t torch.Size([432])\n",
      "features.denseblock2.denselayer6.norm1.running_var \t torch.Size([432])\n",
      "features.denseblock2.denselayer6.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer6.conv1.weight \t torch.Size([192, 432, 1, 1])\n",
      "features.denseblock2.denselayer6.norm2.weight \t torch.Size([192])\n",
      "features.denseblock2.denselayer6.norm2.bias \t torch.Size([192])\n",
      "features.denseblock2.denselayer6.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock2.denselayer6.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock2.denselayer6.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer6.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock2.denselayer7.norm1.weight \t torch.Size([480])\n",
      "features.denseblock2.denselayer7.norm1.bias \t torch.Size([480])\n",
      "features.denseblock2.denselayer7.norm1.running_mean \t torch.Size([480])\n",
      "features.denseblock2.denselayer7.norm1.running_var \t torch.Size([480])\n",
      "features.denseblock2.denselayer7.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer7.conv1.weight \t torch.Size([192, 480, 1, 1])\n",
      "features.denseblock2.denselayer7.norm2.weight \t torch.Size([192])\n",
      "features.denseblock2.denselayer7.norm2.bias \t torch.Size([192])\n",
      "features.denseblock2.denselayer7.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock2.denselayer7.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock2.denselayer7.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer7.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock2.denselayer8.norm1.weight \t torch.Size([528])\n",
      "features.denseblock2.denselayer8.norm1.bias \t torch.Size([528])\n",
      "features.denseblock2.denselayer8.norm1.running_mean \t torch.Size([528])\n",
      "features.denseblock2.denselayer8.norm1.running_var \t torch.Size([528])\n",
      "features.denseblock2.denselayer8.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer8.conv1.weight \t torch.Size([192, 528, 1, 1])\n",
      "features.denseblock2.denselayer8.norm2.weight \t torch.Size([192])\n",
      "features.denseblock2.denselayer8.norm2.bias \t torch.Size([192])\n",
      "features.denseblock2.denselayer8.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock2.denselayer8.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock2.denselayer8.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer8.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock2.denselayer9.norm1.weight \t torch.Size([576])\n",
      "features.denseblock2.denselayer9.norm1.bias \t torch.Size([576])\n",
      "features.denseblock2.denselayer9.norm1.running_mean \t torch.Size([576])\n",
      "features.denseblock2.denselayer9.norm1.running_var \t torch.Size([576])\n",
      "features.denseblock2.denselayer9.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer9.conv1.weight \t torch.Size([192, 576, 1, 1])\n",
      "features.denseblock2.denselayer9.norm2.weight \t torch.Size([192])\n",
      "features.denseblock2.denselayer9.norm2.bias \t torch.Size([192])\n",
      "features.denseblock2.denselayer9.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock2.denselayer9.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock2.denselayer9.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer9.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock2.denselayer10.norm1.weight \t torch.Size([624])\n",
      "features.denseblock2.denselayer10.norm1.bias \t torch.Size([624])\n",
      "features.denseblock2.denselayer10.norm1.running_mean \t torch.Size([624])\n",
      "features.denseblock2.denselayer10.norm1.running_var \t torch.Size([624])\n",
      "features.denseblock2.denselayer10.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer10.conv1.weight \t torch.Size([192, 624, 1, 1])\n",
      "features.denseblock2.denselayer10.norm2.weight \t torch.Size([192])\n",
      "features.denseblock2.denselayer10.norm2.bias \t torch.Size([192])\n",
      "features.denseblock2.denselayer10.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock2.denselayer10.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock2.denselayer10.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer10.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock2.denselayer11.norm1.weight \t torch.Size([672])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.denseblock2.denselayer11.norm1.bias \t torch.Size([672])\n",
      "features.denseblock2.denselayer11.norm1.running_mean \t torch.Size([672])\n",
      "features.denseblock2.denselayer11.norm1.running_var \t torch.Size([672])\n",
      "features.denseblock2.denselayer11.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer11.conv1.weight \t torch.Size([192, 672, 1, 1])\n",
      "features.denseblock2.denselayer11.norm2.weight \t torch.Size([192])\n",
      "features.denseblock2.denselayer11.norm2.bias \t torch.Size([192])\n",
      "features.denseblock2.denselayer11.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock2.denselayer11.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock2.denselayer11.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer11.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock2.denselayer12.norm1.weight \t torch.Size([720])\n",
      "features.denseblock2.denselayer12.norm1.bias \t torch.Size([720])\n",
      "features.denseblock2.denselayer12.norm1.running_mean \t torch.Size([720])\n",
      "features.denseblock2.denselayer12.norm1.running_var \t torch.Size([720])\n",
      "features.denseblock2.denselayer12.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer12.conv1.weight \t torch.Size([192, 720, 1, 1])\n",
      "features.denseblock2.denselayer12.norm2.weight \t torch.Size([192])\n",
      "features.denseblock2.denselayer12.norm2.bias \t torch.Size([192])\n",
      "features.denseblock2.denselayer12.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock2.denselayer12.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock2.denselayer12.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock2.denselayer12.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.transition2.norm.weight \t torch.Size([768])\n",
      "features.transition2.norm.bias \t torch.Size([768])\n",
      "features.transition2.norm.running_mean \t torch.Size([768])\n",
      "features.transition2.norm.running_var \t torch.Size([768])\n",
      "features.transition2.norm.num_batches_tracked \t torch.Size([])\n",
      "features.transition2.conv.weight \t torch.Size([384, 768, 1, 1])\n",
      "features.denseblock3.denselayer1.norm1.weight \t torch.Size([384])\n",
      "features.denseblock3.denselayer1.norm1.bias \t torch.Size([384])\n",
      "features.denseblock3.denselayer1.norm1.running_mean \t torch.Size([384])\n",
      "features.denseblock3.denselayer1.norm1.running_var \t torch.Size([384])\n",
      "features.denseblock3.denselayer1.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer1.conv1.weight \t torch.Size([192, 384, 1, 1])\n",
      "features.denseblock3.denselayer1.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer1.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer1.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer1.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer1.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer1.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer2.norm1.weight \t torch.Size([432])\n",
      "features.denseblock3.denselayer2.norm1.bias \t torch.Size([432])\n",
      "features.denseblock3.denselayer2.norm1.running_mean \t torch.Size([432])\n",
      "features.denseblock3.denselayer2.norm1.running_var \t torch.Size([432])\n",
      "features.denseblock3.denselayer2.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer2.conv1.weight \t torch.Size([192, 432, 1, 1])\n",
      "features.denseblock3.denselayer2.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer2.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer2.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer2.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer2.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer2.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer3.norm1.weight \t torch.Size([480])\n",
      "features.denseblock3.denselayer3.norm1.bias \t torch.Size([480])\n",
      "features.denseblock3.denselayer3.norm1.running_mean \t torch.Size([480])\n",
      "features.denseblock3.denselayer3.norm1.running_var \t torch.Size([480])\n",
      "features.denseblock3.denselayer3.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer3.conv1.weight \t torch.Size([192, 480, 1, 1])\n",
      "features.denseblock3.denselayer3.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer3.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer3.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer3.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer3.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer3.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer4.norm1.weight \t torch.Size([528])\n",
      "features.denseblock3.denselayer4.norm1.bias \t torch.Size([528])\n",
      "features.denseblock3.denselayer4.norm1.running_mean \t torch.Size([528])\n",
      "features.denseblock3.denselayer4.norm1.running_var \t torch.Size([528])\n",
      "features.denseblock3.denselayer4.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer4.conv1.weight \t torch.Size([192, 528, 1, 1])\n",
      "features.denseblock3.denselayer4.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer4.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer4.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer4.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer4.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer4.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer5.norm1.weight \t torch.Size([576])\n",
      "features.denseblock3.denselayer5.norm1.bias \t torch.Size([576])\n",
      "features.denseblock3.denselayer5.norm1.running_mean \t torch.Size([576])\n",
      "features.denseblock3.denselayer5.norm1.running_var \t torch.Size([576])\n",
      "features.denseblock3.denselayer5.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer5.conv1.weight \t torch.Size([192, 576, 1, 1])\n",
      "features.denseblock3.denselayer5.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer5.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer5.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer5.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer5.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer5.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer6.norm1.weight \t torch.Size([624])\n",
      "features.denseblock3.denselayer6.norm1.bias \t torch.Size([624])\n",
      "features.denseblock3.denselayer6.norm1.running_mean \t torch.Size([624])\n",
      "features.denseblock3.denselayer6.norm1.running_var \t torch.Size([624])\n",
      "features.denseblock3.denselayer6.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer6.conv1.weight \t torch.Size([192, 624, 1, 1])\n",
      "features.denseblock3.denselayer6.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer6.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer6.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer6.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer6.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer6.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer7.norm1.weight \t torch.Size([672])\n",
      "features.denseblock3.denselayer7.norm1.bias \t torch.Size([672])\n",
      "features.denseblock3.denselayer7.norm1.running_mean \t torch.Size([672])\n",
      "features.denseblock3.denselayer7.norm1.running_var \t torch.Size([672])\n",
      "features.denseblock3.denselayer7.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer7.conv1.weight \t torch.Size([192, 672, 1, 1])\n",
      "features.denseblock3.denselayer7.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer7.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer7.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer7.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer7.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer7.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer8.norm1.weight \t torch.Size([720])\n",
      "features.denseblock3.denselayer8.norm1.bias \t torch.Size([720])\n",
      "features.denseblock3.denselayer8.norm1.running_mean \t torch.Size([720])\n",
      "features.denseblock3.denselayer8.norm1.running_var \t torch.Size([720])\n",
      "features.denseblock3.denselayer8.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer8.conv1.weight \t torch.Size([192, 720, 1, 1])\n",
      "features.denseblock3.denselayer8.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer8.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer8.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer8.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer8.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer8.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer9.norm1.weight \t torch.Size([768])\n",
      "features.denseblock3.denselayer9.norm1.bias \t torch.Size([768])\n",
      "features.denseblock3.denselayer9.norm1.running_mean \t torch.Size([768])\n",
      "features.denseblock3.denselayer9.norm1.running_var \t torch.Size([768])\n",
      "features.denseblock3.denselayer9.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer9.conv1.weight \t torch.Size([192, 768, 1, 1])\n",
      "features.denseblock3.denselayer9.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer9.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer9.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer9.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer9.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer9.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer10.norm1.weight \t torch.Size([816])\n",
      "features.denseblock3.denselayer10.norm1.bias \t torch.Size([816])\n",
      "features.denseblock3.denselayer10.norm1.running_mean \t torch.Size([816])\n",
      "features.denseblock3.denselayer10.norm1.running_var \t torch.Size([816])\n",
      "features.denseblock3.denselayer10.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer10.conv1.weight \t torch.Size([192, 816, 1, 1])\n",
      "features.denseblock3.denselayer10.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer10.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer10.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer10.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer10.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer10.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer11.norm1.weight \t torch.Size([864])\n",
      "features.denseblock3.denselayer11.norm1.bias \t torch.Size([864])\n",
      "features.denseblock3.denselayer11.norm1.running_mean \t torch.Size([864])\n",
      "features.denseblock3.denselayer11.norm1.running_var \t torch.Size([864])\n",
      "features.denseblock3.denselayer11.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer11.conv1.weight \t torch.Size([192, 864, 1, 1])\n",
      "features.denseblock3.denselayer11.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer11.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer11.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer11.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer11.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer11.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer12.norm1.weight \t torch.Size([912])\n",
      "features.denseblock3.denselayer12.norm1.bias \t torch.Size([912])\n",
      "features.denseblock3.denselayer12.norm1.running_mean \t torch.Size([912])\n",
      "features.denseblock3.denselayer12.norm1.running_var \t torch.Size([912])\n",
      "features.denseblock3.denselayer12.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer12.conv1.weight \t torch.Size([192, 912, 1, 1])\n",
      "features.denseblock3.denselayer12.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer12.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer12.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer12.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer12.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer12.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer13.norm1.weight \t torch.Size([960])\n",
      "features.denseblock3.denselayer13.norm1.bias \t torch.Size([960])\n",
      "features.denseblock3.denselayer13.norm1.running_mean \t torch.Size([960])\n",
      "features.denseblock3.denselayer13.norm1.running_var \t torch.Size([960])\n",
      "features.denseblock3.denselayer13.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer13.conv1.weight \t torch.Size([192, 960, 1, 1])\n",
      "features.denseblock3.denselayer13.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer13.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer13.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer13.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer13.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer13.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer14.norm1.weight \t torch.Size([1008])\n",
      "features.denseblock3.denselayer14.norm1.bias \t torch.Size([1008])\n",
      "features.denseblock3.denselayer14.norm1.running_mean \t torch.Size([1008])\n",
      "features.denseblock3.denselayer14.norm1.running_var \t torch.Size([1008])\n",
      "features.denseblock3.denselayer14.norm1.num_batches_tracked \t torch.Size([])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.denseblock3.denselayer14.conv1.weight \t torch.Size([192, 1008, 1, 1])\n",
      "features.denseblock3.denselayer14.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer14.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer14.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer14.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer14.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer14.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer15.norm1.weight \t torch.Size([1056])\n",
      "features.denseblock3.denselayer15.norm1.bias \t torch.Size([1056])\n",
      "features.denseblock3.denselayer15.norm1.running_mean \t torch.Size([1056])\n",
      "features.denseblock3.denselayer15.norm1.running_var \t torch.Size([1056])\n",
      "features.denseblock3.denselayer15.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer15.conv1.weight \t torch.Size([192, 1056, 1, 1])\n",
      "features.denseblock3.denselayer15.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer15.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer15.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer15.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer15.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer15.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer16.norm1.weight \t torch.Size([1104])\n",
      "features.denseblock3.denselayer16.norm1.bias \t torch.Size([1104])\n",
      "features.denseblock3.denselayer16.norm1.running_mean \t torch.Size([1104])\n",
      "features.denseblock3.denselayer16.norm1.running_var \t torch.Size([1104])\n",
      "features.denseblock3.denselayer16.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer16.conv1.weight \t torch.Size([192, 1104, 1, 1])\n",
      "features.denseblock3.denselayer16.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer16.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer16.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer16.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer16.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer16.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer17.norm1.weight \t torch.Size([1152])\n",
      "features.denseblock3.denselayer17.norm1.bias \t torch.Size([1152])\n",
      "features.denseblock3.denselayer17.norm1.running_mean \t torch.Size([1152])\n",
      "features.denseblock3.denselayer17.norm1.running_var \t torch.Size([1152])\n",
      "features.denseblock3.denselayer17.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer17.conv1.weight \t torch.Size([192, 1152, 1, 1])\n",
      "features.denseblock3.denselayer17.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer17.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer17.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer17.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer17.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer17.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer18.norm1.weight \t torch.Size([1200])\n",
      "features.denseblock3.denselayer18.norm1.bias \t torch.Size([1200])\n",
      "features.denseblock3.denselayer18.norm1.running_mean \t torch.Size([1200])\n",
      "features.denseblock3.denselayer18.norm1.running_var \t torch.Size([1200])\n",
      "features.denseblock3.denselayer18.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer18.conv1.weight \t torch.Size([192, 1200, 1, 1])\n",
      "features.denseblock3.denselayer18.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer18.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer18.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer18.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer18.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer18.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer19.norm1.weight \t torch.Size([1248])\n",
      "features.denseblock3.denselayer19.norm1.bias \t torch.Size([1248])\n",
      "features.denseblock3.denselayer19.norm1.running_mean \t torch.Size([1248])\n",
      "features.denseblock3.denselayer19.norm1.running_var \t torch.Size([1248])\n",
      "features.denseblock3.denselayer19.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer19.conv1.weight \t torch.Size([192, 1248, 1, 1])\n",
      "features.denseblock3.denselayer19.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer19.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer19.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer19.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer19.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer19.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer20.norm1.weight \t torch.Size([1296])\n",
      "features.denseblock3.denselayer20.norm1.bias \t torch.Size([1296])\n",
      "features.denseblock3.denselayer20.norm1.running_mean \t torch.Size([1296])\n",
      "features.denseblock3.denselayer20.norm1.running_var \t torch.Size([1296])\n",
      "features.denseblock3.denselayer20.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer20.conv1.weight \t torch.Size([192, 1296, 1, 1])\n",
      "features.denseblock3.denselayer20.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer20.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer20.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer20.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer20.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer20.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer21.norm1.weight \t torch.Size([1344])\n",
      "features.denseblock3.denselayer21.norm1.bias \t torch.Size([1344])\n",
      "features.denseblock3.denselayer21.norm1.running_mean \t torch.Size([1344])\n",
      "features.denseblock3.denselayer21.norm1.running_var \t torch.Size([1344])\n",
      "features.denseblock3.denselayer21.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer21.conv1.weight \t torch.Size([192, 1344, 1, 1])\n",
      "features.denseblock3.denselayer21.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer21.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer21.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer21.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer21.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer21.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer22.norm1.weight \t torch.Size([1392])\n",
      "features.denseblock3.denselayer22.norm1.bias \t torch.Size([1392])\n",
      "features.denseblock3.denselayer22.norm1.running_mean \t torch.Size([1392])\n",
      "features.denseblock3.denselayer22.norm1.running_var \t torch.Size([1392])\n",
      "features.denseblock3.denselayer22.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer22.conv1.weight \t torch.Size([192, 1392, 1, 1])\n",
      "features.denseblock3.denselayer22.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer22.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer22.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer22.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer22.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer22.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer23.norm1.weight \t torch.Size([1440])\n",
      "features.denseblock3.denselayer23.norm1.bias \t torch.Size([1440])\n",
      "features.denseblock3.denselayer23.norm1.running_mean \t torch.Size([1440])\n",
      "features.denseblock3.denselayer23.norm1.running_var \t torch.Size([1440])\n",
      "features.denseblock3.denselayer23.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer23.conv1.weight \t torch.Size([192, 1440, 1, 1])\n",
      "features.denseblock3.denselayer23.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer23.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer23.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer23.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer23.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer23.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer24.norm1.weight \t torch.Size([1488])\n",
      "features.denseblock3.denselayer24.norm1.bias \t torch.Size([1488])\n",
      "features.denseblock3.denselayer24.norm1.running_mean \t torch.Size([1488])\n",
      "features.denseblock3.denselayer24.norm1.running_var \t torch.Size([1488])\n",
      "features.denseblock3.denselayer24.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer24.conv1.weight \t torch.Size([192, 1488, 1, 1])\n",
      "features.denseblock3.denselayer24.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer24.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer24.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer24.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer24.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer24.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer25.norm1.weight \t torch.Size([1536])\n",
      "features.denseblock3.denselayer25.norm1.bias \t torch.Size([1536])\n",
      "features.denseblock3.denselayer25.norm1.running_mean \t torch.Size([1536])\n",
      "features.denseblock3.denselayer25.norm1.running_var \t torch.Size([1536])\n",
      "features.denseblock3.denselayer25.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer25.conv1.weight \t torch.Size([192, 1536, 1, 1])\n",
      "features.denseblock3.denselayer25.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer25.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer25.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer25.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer25.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer25.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer26.norm1.weight \t torch.Size([1584])\n",
      "features.denseblock3.denselayer26.norm1.bias \t torch.Size([1584])\n",
      "features.denseblock3.denselayer26.norm1.running_mean \t torch.Size([1584])\n",
      "features.denseblock3.denselayer26.norm1.running_var \t torch.Size([1584])\n",
      "features.denseblock3.denselayer26.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer26.conv1.weight \t torch.Size([192, 1584, 1, 1])\n",
      "features.denseblock3.denselayer26.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer26.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer26.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer26.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer26.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer26.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer27.norm1.weight \t torch.Size([1632])\n",
      "features.denseblock3.denselayer27.norm1.bias \t torch.Size([1632])\n",
      "features.denseblock3.denselayer27.norm1.running_mean \t torch.Size([1632])\n",
      "features.denseblock3.denselayer27.norm1.running_var \t torch.Size([1632])\n",
      "features.denseblock3.denselayer27.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer27.conv1.weight \t torch.Size([192, 1632, 1, 1])\n",
      "features.denseblock3.denselayer27.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer27.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer27.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer27.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer27.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer27.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer28.norm1.weight \t torch.Size([1680])\n",
      "features.denseblock3.denselayer28.norm1.bias \t torch.Size([1680])\n",
      "features.denseblock3.denselayer28.norm1.running_mean \t torch.Size([1680])\n",
      "features.denseblock3.denselayer28.norm1.running_var \t torch.Size([1680])\n",
      "features.denseblock3.denselayer28.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer28.conv1.weight \t torch.Size([192, 1680, 1, 1])\n",
      "features.denseblock3.denselayer28.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer28.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer28.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer28.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer28.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer28.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer29.norm1.weight \t torch.Size([1728])\n",
      "features.denseblock3.denselayer29.norm1.bias \t torch.Size([1728])\n",
      "features.denseblock3.denselayer29.norm1.running_mean \t torch.Size([1728])\n",
      "features.denseblock3.denselayer29.norm1.running_var \t torch.Size([1728])\n",
      "features.denseblock3.denselayer29.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer29.conv1.weight \t torch.Size([192, 1728, 1, 1])\n",
      "features.denseblock3.denselayer29.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer29.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer29.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer29.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer29.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer29.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer30.norm1.weight \t torch.Size([1776])\n",
      "features.denseblock3.denselayer30.norm1.bias \t torch.Size([1776])\n",
      "features.denseblock3.denselayer30.norm1.running_mean \t torch.Size([1776])\n",
      "features.denseblock3.denselayer30.norm1.running_var \t torch.Size([1776])\n",
      "features.denseblock3.denselayer30.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer30.conv1.weight \t torch.Size([192, 1776, 1, 1])\n",
      "features.denseblock3.denselayer30.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer30.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer30.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer30.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer30.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer30.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer31.norm1.weight \t torch.Size([1824])\n",
      "features.denseblock3.denselayer31.norm1.bias \t torch.Size([1824])\n",
      "features.denseblock3.denselayer31.norm1.running_mean \t torch.Size([1824])\n",
      "features.denseblock3.denselayer31.norm1.running_var \t torch.Size([1824])\n",
      "features.denseblock3.denselayer31.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer31.conv1.weight \t torch.Size([192, 1824, 1, 1])\n",
      "features.denseblock3.denselayer31.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer31.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer31.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer31.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer31.norm2.num_batches_tracked \t torch.Size([])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.denseblock3.denselayer31.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer32.norm1.weight \t torch.Size([1872])\n",
      "features.denseblock3.denselayer32.norm1.bias \t torch.Size([1872])\n",
      "features.denseblock3.denselayer32.norm1.running_mean \t torch.Size([1872])\n",
      "features.denseblock3.denselayer32.norm1.running_var \t torch.Size([1872])\n",
      "features.denseblock3.denselayer32.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer32.conv1.weight \t torch.Size([192, 1872, 1, 1])\n",
      "features.denseblock3.denselayer32.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer32.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer32.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer32.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer32.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer32.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer33.norm1.weight \t torch.Size([1920])\n",
      "features.denseblock3.denselayer33.norm1.bias \t torch.Size([1920])\n",
      "features.denseblock3.denselayer33.norm1.running_mean \t torch.Size([1920])\n",
      "features.denseblock3.denselayer33.norm1.running_var \t torch.Size([1920])\n",
      "features.denseblock3.denselayer33.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer33.conv1.weight \t torch.Size([192, 1920, 1, 1])\n",
      "features.denseblock3.denselayer33.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer33.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer33.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer33.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer33.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer33.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer34.norm1.weight \t torch.Size([1968])\n",
      "features.denseblock3.denselayer34.norm1.bias \t torch.Size([1968])\n",
      "features.denseblock3.denselayer34.norm1.running_mean \t torch.Size([1968])\n",
      "features.denseblock3.denselayer34.norm1.running_var \t torch.Size([1968])\n",
      "features.denseblock3.denselayer34.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer34.conv1.weight \t torch.Size([192, 1968, 1, 1])\n",
      "features.denseblock3.denselayer34.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer34.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer34.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer34.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer34.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer34.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer35.norm1.weight \t torch.Size([2016])\n",
      "features.denseblock3.denselayer35.norm1.bias \t torch.Size([2016])\n",
      "features.denseblock3.denselayer35.norm1.running_mean \t torch.Size([2016])\n",
      "features.denseblock3.denselayer35.norm1.running_var \t torch.Size([2016])\n",
      "features.denseblock3.denselayer35.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer35.conv1.weight \t torch.Size([192, 2016, 1, 1])\n",
      "features.denseblock3.denselayer35.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer35.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer35.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer35.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer35.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer35.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock3.denselayer36.norm1.weight \t torch.Size([2064])\n",
      "features.denseblock3.denselayer36.norm1.bias \t torch.Size([2064])\n",
      "features.denseblock3.denselayer36.norm1.running_mean \t torch.Size([2064])\n",
      "features.denseblock3.denselayer36.norm1.running_var \t torch.Size([2064])\n",
      "features.denseblock3.denselayer36.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer36.conv1.weight \t torch.Size([192, 2064, 1, 1])\n",
      "features.denseblock3.denselayer36.norm2.weight \t torch.Size([192])\n",
      "features.denseblock3.denselayer36.norm2.bias \t torch.Size([192])\n",
      "features.denseblock3.denselayer36.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock3.denselayer36.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock3.denselayer36.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock3.denselayer36.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.transition3.norm.weight \t torch.Size([2112])\n",
      "features.transition3.norm.bias \t torch.Size([2112])\n",
      "features.transition3.norm.running_mean \t torch.Size([2112])\n",
      "features.transition3.norm.running_var \t torch.Size([2112])\n",
      "features.transition3.norm.num_batches_tracked \t torch.Size([])\n",
      "features.transition3.conv.weight \t torch.Size([1056, 2112, 1, 1])\n",
      "features.denseblock4.denselayer1.norm1.weight \t torch.Size([1056])\n",
      "features.denseblock4.denselayer1.norm1.bias \t torch.Size([1056])\n",
      "features.denseblock4.denselayer1.norm1.running_mean \t torch.Size([1056])\n",
      "features.denseblock4.denselayer1.norm1.running_var \t torch.Size([1056])\n",
      "features.denseblock4.denselayer1.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer1.conv1.weight \t torch.Size([192, 1056, 1, 1])\n",
      "features.denseblock4.denselayer1.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer1.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer1.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer1.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer1.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer1.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer2.norm1.weight \t torch.Size([1104])\n",
      "features.denseblock4.denselayer2.norm1.bias \t torch.Size([1104])\n",
      "features.denseblock4.denselayer2.norm1.running_mean \t torch.Size([1104])\n",
      "features.denseblock4.denselayer2.norm1.running_var \t torch.Size([1104])\n",
      "features.denseblock4.denselayer2.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer2.conv1.weight \t torch.Size([192, 1104, 1, 1])\n",
      "features.denseblock4.denselayer2.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer2.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer2.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer2.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer2.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer2.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer3.norm1.weight \t torch.Size([1152])\n",
      "features.denseblock4.denselayer3.norm1.bias \t torch.Size([1152])\n",
      "features.denseblock4.denselayer3.norm1.running_mean \t torch.Size([1152])\n",
      "features.denseblock4.denselayer3.norm1.running_var \t torch.Size([1152])\n",
      "features.denseblock4.denselayer3.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer3.conv1.weight \t torch.Size([192, 1152, 1, 1])\n",
      "features.denseblock4.denselayer3.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer3.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer3.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer3.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer3.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer3.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer4.norm1.weight \t torch.Size([1200])\n",
      "features.denseblock4.denselayer4.norm1.bias \t torch.Size([1200])\n",
      "features.denseblock4.denselayer4.norm1.running_mean \t torch.Size([1200])\n",
      "features.denseblock4.denselayer4.norm1.running_var \t torch.Size([1200])\n",
      "features.denseblock4.denselayer4.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer4.conv1.weight \t torch.Size([192, 1200, 1, 1])\n",
      "features.denseblock4.denselayer4.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer4.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer4.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer4.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer4.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer4.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer5.norm1.weight \t torch.Size([1248])\n",
      "features.denseblock4.denselayer5.norm1.bias \t torch.Size([1248])\n",
      "features.denseblock4.denselayer5.norm1.running_mean \t torch.Size([1248])\n",
      "features.denseblock4.denselayer5.norm1.running_var \t torch.Size([1248])\n",
      "features.denseblock4.denselayer5.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer5.conv1.weight \t torch.Size([192, 1248, 1, 1])\n",
      "features.denseblock4.denselayer5.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer5.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer5.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer5.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer5.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer5.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer6.norm1.weight \t torch.Size([1296])\n",
      "features.denseblock4.denselayer6.norm1.bias \t torch.Size([1296])\n",
      "features.denseblock4.denselayer6.norm1.running_mean \t torch.Size([1296])\n",
      "features.denseblock4.denselayer6.norm1.running_var \t torch.Size([1296])\n",
      "features.denseblock4.denselayer6.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer6.conv1.weight \t torch.Size([192, 1296, 1, 1])\n",
      "features.denseblock4.denselayer6.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer6.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer6.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer6.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer6.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer6.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer7.norm1.weight \t torch.Size([1344])\n",
      "features.denseblock4.denselayer7.norm1.bias \t torch.Size([1344])\n",
      "features.denseblock4.denselayer7.norm1.running_mean \t torch.Size([1344])\n",
      "features.denseblock4.denselayer7.norm1.running_var \t torch.Size([1344])\n",
      "features.denseblock4.denselayer7.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer7.conv1.weight \t torch.Size([192, 1344, 1, 1])\n",
      "features.denseblock4.denselayer7.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer7.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer7.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer7.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer7.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer7.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer8.norm1.weight \t torch.Size([1392])\n",
      "features.denseblock4.denselayer8.norm1.bias \t torch.Size([1392])\n",
      "features.denseblock4.denselayer8.norm1.running_mean \t torch.Size([1392])\n",
      "features.denseblock4.denselayer8.norm1.running_var \t torch.Size([1392])\n",
      "features.denseblock4.denselayer8.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer8.conv1.weight \t torch.Size([192, 1392, 1, 1])\n",
      "features.denseblock4.denselayer8.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer8.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer8.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer8.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer8.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer8.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer9.norm1.weight \t torch.Size([1440])\n",
      "features.denseblock4.denselayer9.norm1.bias \t torch.Size([1440])\n",
      "features.denseblock4.denselayer9.norm1.running_mean \t torch.Size([1440])\n",
      "features.denseblock4.denselayer9.norm1.running_var \t torch.Size([1440])\n",
      "features.denseblock4.denselayer9.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer9.conv1.weight \t torch.Size([192, 1440, 1, 1])\n",
      "features.denseblock4.denselayer9.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer9.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer9.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer9.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer9.norm2.num_batches_tracked \t torch.Size([])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.denseblock4.denselayer9.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer10.norm1.weight \t torch.Size([1488])\n",
      "features.denseblock4.denselayer10.norm1.bias \t torch.Size([1488])\n",
      "features.denseblock4.denselayer10.norm1.running_mean \t torch.Size([1488])\n",
      "features.denseblock4.denselayer10.norm1.running_var \t torch.Size([1488])\n",
      "features.denseblock4.denselayer10.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer10.conv1.weight \t torch.Size([192, 1488, 1, 1])\n",
      "features.denseblock4.denselayer10.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer10.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer10.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer10.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer10.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer10.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer11.norm1.weight \t torch.Size([1536])\n",
      "features.denseblock4.denselayer11.norm1.bias \t torch.Size([1536])\n",
      "features.denseblock4.denselayer11.norm1.running_mean \t torch.Size([1536])\n",
      "features.denseblock4.denselayer11.norm1.running_var \t torch.Size([1536])\n",
      "features.denseblock4.denselayer11.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer11.conv1.weight \t torch.Size([192, 1536, 1, 1])\n",
      "features.denseblock4.denselayer11.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer11.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer11.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer11.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer11.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer11.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer12.norm1.weight \t torch.Size([1584])\n",
      "features.denseblock4.denselayer12.norm1.bias \t torch.Size([1584])\n",
      "features.denseblock4.denselayer12.norm1.running_mean \t torch.Size([1584])\n",
      "features.denseblock4.denselayer12.norm1.running_var \t torch.Size([1584])\n",
      "features.denseblock4.denselayer12.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer12.conv1.weight \t torch.Size([192, 1584, 1, 1])\n",
      "features.denseblock4.denselayer12.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer12.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer12.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer12.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer12.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer12.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer13.norm1.weight \t torch.Size([1632])\n",
      "features.denseblock4.denselayer13.norm1.bias \t torch.Size([1632])\n",
      "features.denseblock4.denselayer13.norm1.running_mean \t torch.Size([1632])\n",
      "features.denseblock4.denselayer13.norm1.running_var \t torch.Size([1632])\n",
      "features.denseblock4.denselayer13.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer13.conv1.weight \t torch.Size([192, 1632, 1, 1])\n",
      "features.denseblock4.denselayer13.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer13.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer13.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer13.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer13.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer13.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer14.norm1.weight \t torch.Size([1680])\n",
      "features.denseblock4.denselayer14.norm1.bias \t torch.Size([1680])\n",
      "features.denseblock4.denselayer14.norm1.running_mean \t torch.Size([1680])\n",
      "features.denseblock4.denselayer14.norm1.running_var \t torch.Size([1680])\n",
      "features.denseblock4.denselayer14.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer14.conv1.weight \t torch.Size([192, 1680, 1, 1])\n",
      "features.denseblock4.denselayer14.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer14.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer14.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer14.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer14.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer14.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer15.norm1.weight \t torch.Size([1728])\n",
      "features.denseblock4.denselayer15.norm1.bias \t torch.Size([1728])\n",
      "features.denseblock4.denselayer15.norm1.running_mean \t torch.Size([1728])\n",
      "features.denseblock4.denselayer15.norm1.running_var \t torch.Size([1728])\n",
      "features.denseblock4.denselayer15.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer15.conv1.weight \t torch.Size([192, 1728, 1, 1])\n",
      "features.denseblock4.denselayer15.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer15.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer15.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer15.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer15.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer15.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer16.norm1.weight \t torch.Size([1776])\n",
      "features.denseblock4.denselayer16.norm1.bias \t torch.Size([1776])\n",
      "features.denseblock4.denselayer16.norm1.running_mean \t torch.Size([1776])\n",
      "features.denseblock4.denselayer16.norm1.running_var \t torch.Size([1776])\n",
      "features.denseblock4.denselayer16.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer16.conv1.weight \t torch.Size([192, 1776, 1, 1])\n",
      "features.denseblock4.denselayer16.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer16.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer16.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer16.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer16.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer16.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer17.norm1.weight \t torch.Size([1824])\n",
      "features.denseblock4.denselayer17.norm1.bias \t torch.Size([1824])\n",
      "features.denseblock4.denselayer17.norm1.running_mean \t torch.Size([1824])\n",
      "features.denseblock4.denselayer17.norm1.running_var \t torch.Size([1824])\n",
      "features.denseblock4.denselayer17.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer17.conv1.weight \t torch.Size([192, 1824, 1, 1])\n",
      "features.denseblock4.denselayer17.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer17.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer17.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer17.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer17.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer17.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer18.norm1.weight \t torch.Size([1872])\n",
      "features.denseblock4.denselayer18.norm1.bias \t torch.Size([1872])\n",
      "features.denseblock4.denselayer18.norm1.running_mean \t torch.Size([1872])\n",
      "features.denseblock4.denselayer18.norm1.running_var \t torch.Size([1872])\n",
      "features.denseblock4.denselayer18.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer18.conv1.weight \t torch.Size([192, 1872, 1, 1])\n",
      "features.denseblock4.denselayer18.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer18.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer18.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer18.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer18.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer18.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer19.norm1.weight \t torch.Size([1920])\n",
      "features.denseblock4.denselayer19.norm1.bias \t torch.Size([1920])\n",
      "features.denseblock4.denselayer19.norm1.running_mean \t torch.Size([1920])\n",
      "features.denseblock4.denselayer19.norm1.running_var \t torch.Size([1920])\n",
      "features.denseblock4.denselayer19.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer19.conv1.weight \t torch.Size([192, 1920, 1, 1])\n",
      "features.denseblock4.denselayer19.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer19.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer19.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer19.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer19.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer19.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer20.norm1.weight \t torch.Size([1968])\n",
      "features.denseblock4.denselayer20.norm1.bias \t torch.Size([1968])\n",
      "features.denseblock4.denselayer20.norm1.running_mean \t torch.Size([1968])\n",
      "features.denseblock4.denselayer20.norm1.running_var \t torch.Size([1968])\n",
      "features.denseblock4.denselayer20.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer20.conv1.weight \t torch.Size([192, 1968, 1, 1])\n",
      "features.denseblock4.denselayer20.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer20.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer20.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer20.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer20.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer20.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer21.norm1.weight \t torch.Size([2016])\n",
      "features.denseblock4.denselayer21.norm1.bias \t torch.Size([2016])\n",
      "features.denseblock4.denselayer21.norm1.running_mean \t torch.Size([2016])\n",
      "features.denseblock4.denselayer21.norm1.running_var \t torch.Size([2016])\n",
      "features.denseblock4.denselayer21.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer21.conv1.weight \t torch.Size([192, 2016, 1, 1])\n",
      "features.denseblock4.denselayer21.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer21.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer21.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer21.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer21.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer21.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer22.norm1.weight \t torch.Size([2064])\n",
      "features.denseblock4.denselayer22.norm1.bias \t torch.Size([2064])\n",
      "features.denseblock4.denselayer22.norm1.running_mean \t torch.Size([2064])\n",
      "features.denseblock4.denselayer22.norm1.running_var \t torch.Size([2064])\n",
      "features.denseblock4.denselayer22.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer22.conv1.weight \t torch.Size([192, 2064, 1, 1])\n",
      "features.denseblock4.denselayer22.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer22.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer22.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer22.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer22.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer22.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer23.norm1.weight \t torch.Size([2112])\n",
      "features.denseblock4.denselayer23.norm1.bias \t torch.Size([2112])\n",
      "features.denseblock4.denselayer23.norm1.running_mean \t torch.Size([2112])\n",
      "features.denseblock4.denselayer23.norm1.running_var \t torch.Size([2112])\n",
      "features.denseblock4.denselayer23.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer23.conv1.weight \t torch.Size([192, 2112, 1, 1])\n",
      "features.denseblock4.denselayer23.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer23.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer23.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer23.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer23.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer23.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.denseblock4.denselayer24.norm1.weight \t torch.Size([2160])\n",
      "features.denseblock4.denselayer24.norm1.bias \t torch.Size([2160])\n",
      "features.denseblock4.denselayer24.norm1.running_mean \t torch.Size([2160])\n",
      "features.denseblock4.denselayer24.norm1.running_var \t torch.Size([2160])\n",
      "features.denseblock4.denselayer24.norm1.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer24.conv1.weight \t torch.Size([192, 2160, 1, 1])\n",
      "features.denseblock4.denselayer24.norm2.weight \t torch.Size([192])\n",
      "features.denseblock4.denselayer24.norm2.bias \t torch.Size([192])\n",
      "features.denseblock4.denselayer24.norm2.running_mean \t torch.Size([192])\n",
      "features.denseblock4.denselayer24.norm2.running_var \t torch.Size([192])\n",
      "features.denseblock4.denselayer24.norm2.num_batches_tracked \t torch.Size([])\n",
      "features.denseblock4.denselayer24.conv2.weight \t torch.Size([48, 192, 3, 3])\n",
      "features.norm5.weight \t torch.Size([2208])\n",
      "features.norm5.bias \t torch.Size([2208])\n",
      "features.norm5.running_mean \t torch.Size([2208])\n",
      "features.norm5.running_var \t torch.Size([2208])\n",
      "features.norm5.num_batches_tracked \t torch.Size([])\n",
      "classifier.weight \t torch.Size([2, 2208])\n",
      "classifier.bias \t torch.Size([2])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483]}]\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DenseNet\n",
      "Epoch : 000 \n",
      "Training: Loss: 0.6767, Accuracy: 57.2769%, \n",
      "Validation : Loss : 5.6540, Accuracy: 35.4053%, \n",
      " Time (train+val): 31074.5262s\n",
      "  **\n",
      "Saved model DenseNet with loss 5.654006976881399\n"
     ]
    }
   ],
   "source": [
    "# history = []\n",
    "# # Initialize the variable storing accuracy and loss (with max/min values)\n",
    "# history.append([1.0, 1.0, 0.0, 0.0])\n",
    "# best_loss_on_val = np.Infinity\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "#     print(\"Epoch: {}/{}\".format(epoch, epochs))\n",
    "     \n",
    "    # Set to training mode\n",
    "    model.train()\n",
    "     \n",
    "    # Loss and Accuracy within the epoch\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "     \n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    " \n",
    "    # Iterate through all batches of training data\n",
    "    for i, (inputs, labels) in enumerate(train_data):\n",
    " \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "         \n",
    "        # Clean existing gradients\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "#         print(\"predictions\")\n",
    "#         print(outputs)\n",
    "         \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "         \n",
    "        # Backpropagate the gradients\n",
    "        loss.backward()\n",
    "         \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "         \n",
    "        # Compute the total loss for the batch and add it to train_loss\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "         \n",
    "        # Compute the accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "#         print(\"ret\")\n",
    "#         print(ret)\n",
    "#         print(\"predictions\")\n",
    "#         print(predictions)\n",
    "\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "#         print(\"correct counts\")\n",
    "#         print(correct_counts)\n",
    "         \n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "#         print(\"acc\")\n",
    "#         print(acc)\n",
    "         \n",
    "        # Compute total accuracy in the whole batch and add to train_acc\n",
    "        train_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "         \n",
    "#         print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "\n",
    "        # Break \"train_data batch\" for loop\n",
    "#         break\n",
    "    \n",
    "        \n",
    "\n",
    "    # Validation is carried out in each epoch immediately after the training loop\n",
    "    # Validation - No gradient tracking needed\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Validation loop\n",
    "        # Iterate through all batches of validation data\n",
    "        for j, (inputs, labels) in enumerate(valid_data):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute the total loss for the batch and add it to valid_loss\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            # Compute total accuracy in the whole batch and add to valid_acc\n",
    "            valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "#             print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "\n",
    "            # Break \"valid_data batch\" for loop\n",
    "#             break\n",
    "\n",
    "    # Find average training loss and training accuracy\n",
    "    avg_train_loss = train_loss/train_data_size\n",
    "    avg_train_acc = train_acc/float(train_data_size)\n",
    "\n",
    "    # Find average training loss and training accuracy\n",
    "    avg_valid_loss = valid_loss/valid_data_size\n",
    "    avg_valid_acc = valid_acc/float(valid_data_size)\n",
    "\n",
    "    history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"Epoch : {:03d} \\nTraining: Loss: {:.4f}, Accuracy: {:.4f}%, \\nValidation : Loss : {:.4f}, Accuracy: {:.4f}%, \\n Time (train+val): {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
    "\n",
    "    \n",
    "    # Source: https://github.com/choosehappy/PytorchDigitalPathology/blob/master/classification_lymphoma_densenet/train_densenet.ipynb\n",
    "    # If current loss is the best we've seen, save model state with all variables\n",
    "    # necessary for recreation\n",
    "    if avg_valid_loss < best_loss_on_val:\n",
    "        best_loss_on_val = avg_valid_loss\n",
    "        print(\"  **\")\n",
    "        state = {'epoch': epoch + 1,\n",
    "         'model_dict': model.state_dict(),\n",
    "         'optim_dict': optimizer.state_dict(),\n",
    "         'best_loss_on_val': best_loss_on_val}\n",
    "\n",
    "        torch.save(state, f\"{model_name}_best_model.pth\")\n",
    "        print(f\"Saved model {model_name} with loss {avg_valid_loss}\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "    \n",
    "    # Stop \"epoch\" for\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: DenseNet\n",
    "Epoch : 000 \n",
    "Training: Loss: 0.6769, Accuracy: 57.1705%, \n",
    "Validation : Loss : 5.7904, Accuracy: 35.1516%, \n",
    " Time (train+val): 33245.9189s\n",
    "  **\n",
    "Saved model DenseNet with loss 5.790416442187604\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the accuracy and loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# accuracy plots\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(range(len(history_loaded)), [x[3] for x in history_loaded] , color='green', label='validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('outputs/initial_validation_accuracy.png')\n",
    "plt.show()\n",
    " \n",
    "# loss plots\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(range(len(history_loaded)), [x[1] for x in history_loaded], color='orange', label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('outputs/initial_training_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print model and optimizer parameters after training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in alexnet.state_dict():\n",
    "    print(param_tensor, \"\\t\", alexnet.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source:\n",
    "# https://debuggercafe.com/effective-model-saving-and-resuming-training-in-pytorch/\n",
    "# save model checkpoint\n",
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': alexnet.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion,\n",
    "            }, 'outputs/alexnet_trained.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the model before loading the previously saved one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "alexnet_loaded = torchvision.models.alexnet(pretrained=True)\n",
    "# Initialize optimizer  before loading optimizer state_dict\n",
    "optimizer_loaded = optim.Adam(alexnet_loaded.parameters()) \n",
    "\n",
    "# Updating the second classifier\n",
    "alexnet_loaded.classifier[4] = nn.Linear(4096,1024)\n",
    "\n",
    "#Updating the third and the last classifier that is the output layer of the network. Make sure to have 10 output nodes if we are going to get 10 class labels through our model.\n",
    "# AdVo: i will have a binary classification , thus only 2 output nodes\n",
    "alexnet_loaded.classifier[6] = nn.Linear(1024, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the saved model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained optimizer state_dict loaded...\n",
      "Trained model loss function loaded...\n",
      "Previously trained for 3 number of epochs...\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "checkpoint = torch.load('outputs/alexnet_trained.pth')\n",
    "\n",
    "# load model weights state_dict\n",
    "alexnet_loaded.load_state_dict(checkpoint['model_state_dict'])\n",
    "print('Previously trained model weights state_dict loaded...')\n",
    "\n",
    "# load trained optimizer state_dict\n",
    "optimizer_loaded.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "print('Previously trained optimizer state_dict loaded...')\n",
    "\n",
    "epochs = checkpoint['epoch']\n",
    "\n",
    "# load the criterion\n",
    "criterion_loaded = checkpoint['loss']\n",
    "print('Trained model loss function loaded...')\n",
    "\n",
    "print(f\"Previously trained for {epochs} number of epochs...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print loaded model and optimizer parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in alexnet_loaded.state_dict():\n",
    "    print(param_tensor, \"\\t\", alexnet_loaded.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer_loaded.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer_loaded.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize parameters for second phase of training (optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 5 more epochs...\n"
     ]
    }
   ],
   "source": [
    "# learning parameters\n",
    "batch_size = 64\n",
    "new_epochs = 5\n",
    "# lr = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# train for more epochs\n",
    "epochs = new_epochs\n",
    "print(f\"Train for {epochs} more epochs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 000, Training: Loss: 0.0290, Accuracy: 99.0720%, \n",
      "\t\tValidation : Loss : 192.5187, Accuracy: 61.5314%, \n",
      " Time (train+val): 4045.6382s\n",
      "Epoch : 001, Training: Loss: 0.0275, Accuracy: 99.1176%, \n",
      "\t\tValidation : Loss : 210.6774, Accuracy: 62.1864%, \n",
      " Time (train+val): 3821.8940s\n",
      "Epoch : 002, Training: Loss: 0.0420, Accuracy: 98.6470%, \n",
      "\t\tValidation : Loss : 104.0322, Accuracy: 46.4879%, \n",
      " Time (train+val): 3726.6068s\n",
      "Epoch : 003, Training: Loss: 0.0308, Accuracy: 98.9854%, \n",
      "\t\tValidation : Loss : 48.8112, Accuracy: 56.6813%, \n",
      " Time (train+val): 4840.4254s\n",
      "Epoch : 004, Training: Loss: 0.0338, Accuracy: 98.9233%, \n",
      "\t\tValidation : Loss : 18.8629, Accuracy: 57.5315%, \n",
      " Time (train+val): 4220.9801s\n"
     ]
    }
   ],
   "source": [
    "history_loaded = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "#     print(\"Epoch: {}/{}\".format(epoch, epochs))\n",
    "     \n",
    "    # Set to training mode\n",
    "    alexnet_loaded.train()\n",
    "     \n",
    "    # Loss and Accuracy within the epoch\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "     \n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    " \n",
    "    # Iterate through all batches of training data\n",
    "    for i, (inputs, labels) in enumerate(train_data):\n",
    " \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "         \n",
    "        # Clean existing gradients\n",
    "        optimizer_loaded.zero_grad()\n",
    "         \n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = alexnet_loaded(inputs)\n",
    "        \n",
    "#         print(\"predictions\")\n",
    "#         print(outputs)\n",
    "         \n",
    "        # Compute loss\n",
    "        loss = criterion_loaded(outputs, labels)\n",
    "         \n",
    "        # Backpropagate the gradients\n",
    "        loss.backward()\n",
    "         \n",
    "        # Update the parameters\n",
    "        optimizer_loaded.step()\n",
    "         \n",
    "        # Compute the total loss for the batch and add it to train_loss\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "         \n",
    "        # Compute the accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "#         print(\"ret\")\n",
    "#         print(ret)\n",
    "#         print(\"predictions\")\n",
    "#         print(predictions)\n",
    "\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "#         print(\"correct counts\")\n",
    "#         print(correct_counts)\n",
    "         \n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "#         print(\"acc\")\n",
    "#         print(acc)\n",
    "         \n",
    "        # Compute total accuracy in the whole batch and add to train_acc\n",
    "        train_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "         \n",
    "#         print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "        \n",
    "\n",
    "    # Validation is carried out in each epoch immediately after the training loop\n",
    "    # Validation - No gradient tracking needed\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Set to evaluation mode\n",
    "        alexnet_loaded.eval()\n",
    "\n",
    "        # Validation loop\n",
    "        # Iterate through all batches of validation data\n",
    "        for j, (inputs, labels) in enumerate(valid_data):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = alexnet_loaded(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion_loaded(outputs, labels)\n",
    "\n",
    "            # Compute the total loss for the batch and add it to valid_loss\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            # Compute total accuracy in the whole batch and add to valid_acc\n",
    "            valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "#             print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "\n",
    "    # Find average training loss and training accuracy\n",
    "    avg_train_loss = train_loss/train_data_size\n",
    "    avg_train_acc = train_acc/float(train_data_size)\n",
    "\n",
    "    # Find average training loss and training accuracy\n",
    "    avg_valid_loss = valid_loss/valid_data_size\n",
    "    avg_valid_acc = valid_acc/float(valid_data_size)\n",
    "\n",
    "    history_loaded.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, \\n Time (train+val): {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate model based on test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test : Loss : 72.5857, Accuracy: 66.7013%, \n",
      " Time : 380.4901s\n"
     ]
    }
   ],
   "source": [
    "test_start = time.time()\n",
    "\n",
    "# Loss and Accuracy within the epoch\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "\n",
    "# Validation is carried out in each epoch immediately after the training loop\n",
    "# Validation - No gradient tracking needed\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Set to evaluation mode\n",
    "    alexnet_loaded.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    # Iterate through all batches of validation data\n",
    "    for j, (inputs, labels) in enumerate(test_data):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = alexnet_loaded(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion_loaded(outputs, labels)\n",
    "\n",
    "        # Compute the total loss for the batch and add it to valid_loss\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "        # Compute total accuracy in the whole batch and add to valid_acc\n",
    "        test_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "#             print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "\n",
    "\n",
    "# Find average testing loss and accuracy\n",
    "avg_test_loss = test_loss/test_data_size\n",
    "avg_test_acc = test_acc/float(test_data_size)\n",
    "\n",
    "# history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "\n",
    "test_end = time.time()\n",
    "\n",
    "print(\"Test : Loss : {:.4f}, Accuracy: {:.4f}%, \\n Time : {:.4f}s\".format(avg_test_loss, avg_test_acc*100, test_end-test_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_test = []\n",
    "\n",
    "alexnet.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_data:\n",
    "#         X_batch = X_batch.to(device)\n",
    "        y_test_pred = alexnet(X_batch[0])\n",
    "#         y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        labels = X_batch[1]\n",
    "        y_test.append(labels) \n",
    "#         print(type(X_batch[0]))\n",
    "#         break\n",
    "        \n",
    "        \n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "y_test = [a.squeeze().tolist() for a in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
