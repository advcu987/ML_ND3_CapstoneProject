{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: \n",
    "# https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/\n",
    "\n",
    "# Applying Transforms to the Data\n",
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "#         transforms.RandomRotation(degrees=15),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "# Set train and valid directory paths\n",
    "train_directory = '/home/advo/PycharmProjects/ML_ND3_CapstoneProject/Dataset_small/patches/train'\n",
    "valid_directory = '/home/advo/PycharmProjects/ML_ND3_CapstoneProject/Dataset_small/patches/valid'\n",
    "test_directory = '/home/advo/PycharmProjects/ML_ND3_CapstoneProject/Dataset_small/patches/test'\n",
    " \n",
    "# Batch size\n",
    "bs = 64\n",
    " \n",
    "# Number of classes\n",
    "num_classes = 2\n",
    " \n",
    "# Load Data from folders\n",
    "data = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid']),\n",
    "    'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n",
    "}\n",
    " \n",
    "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
    "train_data_size = len(data['train'])\n",
    "valid_data_size = len(data['valid'])\n",
    "test_data_size = len(data['test'])\n",
    " \n",
    "# Create iterators for the Data loaded using DataLoader module\n",
    "train_data = torch.utils.data.DataLoader(data['train'], batch_size=bs, shuffle=True)\n",
    "valid_data = torch.utils.data.DataLoader(data['valid'], batch_size=bs, shuffle=True)\n",
    "test_data = torch.utils.data.DataLoader(data['test'], batch_size=bs, shuffle=True)\n",
    " \n",
    "# Print the train, validation and test set data sizes\n",
    "print(f\"Train size: {train_data_size} \\nValidation size: {valid_data_size} \\nTest size: {test_data_size}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained AlexNet Model\n",
    "alexnet = torchvision.models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: \n",
    "# https://analyticsindiamag.com/implementing-alexnet-using-pytorch-as-a-transfer-learning-model-in-multi-class-classification/\n",
    "\n",
    "# Updating the second classifier\n",
    "alexnet.classifier[4] = nn.Linear(4096,1024)\n",
    "\n",
    "#Updating the third and the last classifier that is the output layer of the network. Make sure to have 10 output nodes if we are going to get 10 class labels through our model.\n",
    "# AdVo: i will have a binary classification , thus only 2 output nodes\n",
    "alexnet.classifier[6] = nn.Linear(1024, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function: Cross Entropy Loss\n",
    "\n",
    "Note: Improvement option by adding weight class check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: \n",
    "# https://github.com/choosehappy/PytorchDigitalPathology/blob/master/visualization_densenet/train_densenet.ipynb\n",
    "\n",
    "# we have the ability to weight individual classes, in this case we'll do so based on their presense in the trainingset\n",
    "# to avoid biasing any particular class\n",
    "# nclasses = dataset[\"train\"].classsizes.shape[0]\n",
    "# class_weight=dataset[\"train\"].classsizes\n",
    "# class_weight = torch.from_numpy(1-class_weight/class_weight.sum()).type('torch.FloatTensor').to(device)\n",
    "\n",
    "# print(class_weight) #show final used weights, make sure that they're reasonable before continouing\n",
    "\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss(weight = class_weight)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: \n",
    "# https://github.com/choosehappy/PytorchDigitalPathology/blob/master/visualization_densenet/train_densenet.ipynb\n",
    "\n",
    "# adam is going to be the most robust, though perhaps not the best performing, typically a good place to start\n",
    "optimizer = optim.Adam(alexnet.parameters()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(10):  # loop over the dataset multiple times\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(train_data, 0):\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         output = alexnet(inputs)\n",
    "#         loss = criterion(output, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training of AlexNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"You are running on the following device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print model and optimizer parameters before training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in alexnet.state_dict():\n",
    "    print(param_tensor, \"\\t\", alexnet.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "history.append([1.0, 1.0, 0.0, 0.0])\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "#     print(\"Epoch: {}/{}\".format(epoch, epochs))\n",
    "     \n",
    "    # Set to training mode\n",
    "    alexnet.train()\n",
    "     \n",
    "    # Loss and Accuracy within the epoch\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "     \n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    " \n",
    "    # Iterate through all batches of training data\n",
    "    for i, (inputs, labels) in enumerate(train_data):\n",
    " \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "         \n",
    "        # Clean existing gradients\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = alexnet(inputs)\n",
    "        \n",
    "#         print(\"predictions\")\n",
    "#         print(outputs)\n",
    "         \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "         \n",
    "        # Backpropagate the gradients\n",
    "        loss.backward()\n",
    "         \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "         \n",
    "        # Compute the total loss for the batch and add it to train_loss\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "         \n",
    "        # Compute the accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "#         print(\"ret\")\n",
    "#         print(ret)\n",
    "#         print(\"predictions\")\n",
    "#         print(predictions)\n",
    "\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "#         print(\"correct counts\")\n",
    "#         print(correct_counts)\n",
    "         \n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "#         print(\"acc\")\n",
    "#         print(acc)\n",
    "         \n",
    "        # Compute total accuracy in the whole batch and add to train_acc\n",
    "        train_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "         \n",
    "#         print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "        \n",
    "\n",
    "    # Validation is carried out in each epoch immediately after the training loop\n",
    "    # Validation - No gradient tracking needed\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Set to evaluation mode\n",
    "        alexnet.eval()\n",
    "\n",
    "        # Validation loop\n",
    "        # Iterate through all batches of validation data\n",
    "        for j, (inputs, labels) in enumerate(valid_data):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = alexnet(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute the total loss for the batch and add it to valid_loss\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            # Compute total accuracy in the whole batch and add to valid_acc\n",
    "            valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "#             print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "\n",
    "    # Find average training loss and training accuracy\n",
    "    avg_train_loss = train_loss/train_data_size\n",
    "    avg_train_acc = train_acc/float(train_data_size)\n",
    "\n",
    "    # Find average training loss and training accuracy\n",
    "    avg_valid_loss = valid_loss/valid_data_size\n",
    "    avg_valid_acc = valid_acc/float(valid_data_size)\n",
    "\n",
    "    history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, \\n Time (train+val): {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
    "\n",
    "    # Stop \"epoch\" for\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source:\n",
    "# https://debuggercafe.com/effective-model-saving-and-resuming-training-in-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the accuracy and loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# accuracy plots\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(range(len(history)), [x[3] for x in history] , color='green', label='validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('outputs/initial_validation_accuracy.png')\n",
    "plt.show()\n",
    " \n",
    "# loss plots\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(range(len(history)), [x[1] for x in history], color='orange', label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('outputs/initial_training_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print model and optimizer parameters after training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in alexnet.state_dict():\n",
    "    print(param_tensor, \"\\t\", alexnet.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model checkpoint\n",
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': alexnet.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion,\n",
    "            }, 'outputs/alexnet_trained.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the model before loading the previously saved one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "alexnet_loaded = torchvision.models.alexnet(pretrained=True)\n",
    "# Initialize optimizer  before loading optimizer state_dict\n",
    "optimizer_loaded = optim.Adam(alexnet_loaded.parameters()) \n",
    "\n",
    "# Updating the second classifier\n",
    "alexnet_loaded.classifier[4] = nn.Linear(4096,1024)\n",
    "\n",
    "#Updating the third and the last classifier that is the output layer of the network. Make sure to have 10 output nodes if we are going to get 10 class labels through our model.\n",
    "# AdVo: i will have a binary classification , thus only 2 output nodes\n",
    "alexnet_loaded.classifier[6] = nn.Linear(1024, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the saved model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained optimizer state_dict loaded...\n",
      "Trained model loss function loaded...\n",
      "Previously trained for 3 number of epochs...\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "checkpoint = torch.load('outputs/alexnet_trained.pth')\n",
    "\n",
    "# load model weights state_dict\n",
    "alexnet_loaded.load_state_dict(checkpoint['model_state_dict'])\n",
    "print('Previously trained model weights state_dict loaded...')\n",
    "\n",
    "# load trained optimizer state_dict\n",
    "optimizer_loaded.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "print('Previously trained optimizer state_dict loaded...')\n",
    "\n",
    "epochs = checkpoint['epoch']\n",
    "\n",
    "# load the criterion\n",
    "criterion_loaded = checkpoint['loss']\n",
    "print('Trained model loss function loaded...')\n",
    "\n",
    "print(f\"Previously trained for {epochs} number of epochs...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print loaded model and optimizer parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in alexnet_loaded.state_dict():\n",
    "    print(param_tensor, \"\\t\", alexnet_loaded.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer_loaded.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer_loaded.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize parameters for second phase of training (optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 5 more epochs...\n"
     ]
    }
   ],
   "source": [
    "# learning parameters\n",
    "batch_size = 64\n",
    "new_epochs = 5\n",
    "# lr = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# train for more epochs\n",
    "epochs = new_epochs\n",
    "print(f\"Train for {epochs} more epochs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 000, Training: Loss: 0.0413, Accuracy: 98.6014%, \n",
      "\t\tValidation : Loss : 2.0769, Accuracy: 51.7672%, \n",
      " Time (train+val): 3939.7182s\n",
      "Epoch : 001, Training: Loss: 0.0380, Accuracy: 98.7832%, \n",
      "\t\tValidation : Loss : 2.8047, Accuracy: 59.0813%, \n",
      " Time (train+val): 3840.8866s\n",
      "Epoch : 002, Training: Loss: 0.0391, Accuracy: 98.7283%, \n",
      "\t\tValidation : Loss : 5.5947, Accuracy: 65.4839%, \n",
      " Time (train+val): 3871.7762s\n",
      "Epoch : 003, Training: Loss: 0.0329, Accuracy: 98.9253%, \n",
      "\t\tValidation : Loss : 52.9127, Accuracy: 60.2185%, \n",
      " Time (train+val): 3881.5776s\n",
      "Epoch : 004, Training: Loss: 0.0347, Accuracy: 98.8757%, \n",
      "\t\tValidation : Loss : 61.3909, Accuracy: 67.7445%, \n",
      " Time (train+val): 3824.6950s\n"
     ]
    }
   ],
   "source": [
    "history_loaded = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "#     print(\"Epoch: {}/{}\".format(epoch, epochs))\n",
    "     \n",
    "    # Set to training mode\n",
    "    alexnet_loaded.train()\n",
    "     \n",
    "    # Loss and Accuracy within the epoch\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "     \n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    " \n",
    "    # Iterate through all batches of training data\n",
    "    for i, (inputs, labels) in enumerate(train_data):\n",
    " \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "         \n",
    "        # Clean existing gradients\n",
    "        optimizer_loaded.zero_grad()\n",
    "         \n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = alexnet_loaded(inputs)\n",
    "        \n",
    "#         print(\"predictions\")\n",
    "#         print(outputs)\n",
    "         \n",
    "        # Compute loss\n",
    "        loss = criterion_loaded(outputs, labels)\n",
    "         \n",
    "        # Backpropagate the gradients\n",
    "        loss.backward()\n",
    "         \n",
    "        # Update the parameters\n",
    "        optimizer_loaded.step()\n",
    "         \n",
    "        # Compute the total loss for the batch and add it to train_loss\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "         \n",
    "        # Compute the accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "#         print(\"ret\")\n",
    "#         print(ret)\n",
    "#         print(\"predictions\")\n",
    "#         print(predictions)\n",
    "\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "#         print(\"correct counts\")\n",
    "#         print(correct_counts)\n",
    "         \n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "#         print(\"acc\")\n",
    "#         print(acc)\n",
    "         \n",
    "        # Compute total accuracy in the whole batch and add to train_acc\n",
    "        train_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "         \n",
    "#         print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "        \n",
    "\n",
    "    # Validation is carried out in each epoch immediately after the training loop\n",
    "    # Validation - No gradient tracking needed\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Set to evaluation mode\n",
    "        alexnet_loaded.eval()\n",
    "\n",
    "        # Validation loop\n",
    "        # Iterate through all batches of validation data\n",
    "        for j, (inputs, labels) in enumerate(valid_data):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = alexnet_loaded(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion_loaded(outputs, labels)\n",
    "\n",
    "            # Compute the total loss for the batch and add it to valid_loss\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            # Compute total accuracy in the whole batch and add to valid_acc\n",
    "            valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "#             print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "\n",
    "    # Find average training loss and training accuracy\n",
    "    avg_train_loss = train_loss/train_data_size\n",
    "    avg_train_acc = train_acc/float(train_data_size)\n",
    "\n",
    "    # Find average training loss and training accuracy\n",
    "    avg_valid_loss = valid_loss/valid_data_size\n",
    "    avg_valid_acc = valid_acc/float(valid_data_size)\n",
    "\n",
    "    history_loaded.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, \\n Time (train+val): {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate model based on test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test : Loss : 72.5857, Accuracy: 66.7013%, \n",
      " Time : 380.4901s\n"
     ]
    }
   ],
   "source": [
    "test_start = time.time()\n",
    "\n",
    "# Loss and Accuracy within the epoch\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "\n",
    "# Validation is carried out in each epoch immediately after the training loop\n",
    "# Validation - No gradient tracking needed\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Set to evaluation mode\n",
    "    alexnet_loaded.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    # Iterate through all batches of validation data\n",
    "    for j, (inputs, labels) in enumerate(test_data):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = alexnet_loaded(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion_loaded(outputs, labels)\n",
    "\n",
    "        # Compute the total loss for the batch and add it to valid_loss\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "        # Compute total accuracy in the whole batch and add to valid_acc\n",
    "        test_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "#             print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "\n",
    "\n",
    "# Find average testing loss and accuracy\n",
    "avg_test_loss = test_loss/test_data_size\n",
    "avg_test_acc = test_acc/float(test_data_size)\n",
    "\n",
    "# history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "\n",
    "test_end = time.time()\n",
    "\n",
    "print(\"Test : Loss : {:.4f}, Accuracy: {:.4f}%, \\n Time : {:.4f}s\".format(avg_test_loss, avg_test_acc*100, test_end-test_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_test = []\n",
    "\n",
    "alexnet.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_data:\n",
    "#         X_batch = X_batch.to(device)\n",
    "        y_test_pred = alexnet(X_batch[0])\n",
    "#         y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        labels = X_batch[1]\n",
    "        y_test.append(labels) \n",
    "#         print(type(X_batch[0]))\n",
    "#         break\n",
    "        \n",
    "        \n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "y_test = [a.squeeze().tolist() for a in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
